| 周次         | 学习重点              | 核心内容                                                     | 实战任务                                                     |
| ------------ | --------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **第 1 周**  | 推荐系统入门          | - 推荐系统概览：协同过滤 vs. 基于内容- 用户—物品相似度计算（余弦、皮尔逊）- PyTorch 数据管道与训练循环基础 | - 在 MovieLens 100K 上用纯 Python （或 PyTorch Tensor）实现 UserCF 和 ItemCF- 打通数据加载 → 相似度计算 → Top-K 推荐流程 |
| **第 2 周**  | 隐式反馈与矩阵分解    | - 隐式反馈 vs. 显式反馈的区别- 矩阵分解（SGD、ALS）原理与实现- 评价指标：Precision@K、Recall@K、MAP | - 从头实现隐式 MF（SGD 版本），与 Spotlight/RecBole 内置实现对比效果- 绘制训练损失与评估指标随 epoch 变化曲线 |
| **第 3 周**  | 深度排序模型          | - Wide & Deep、Neural CF（NCF）架构- Embedding 层详解- 负采样策略 | - 基于 PyTorch 搭建一个简单的 NCF，训练并与 MF 在同一数据集上对比- 实现并调优不同负采样比例，观察对推荐质量的影响 |
| **第 4 周**  | 因果推断基础          | - 因果图（DAG）与结构化方程模型（SEM）- 混杂、选择偏差与倒因果- 平均处理效应（ATE）、倾向得分匹配（PSM） | - 用 DoWhy（或 EconML）做一个简单的 A/B 实验分析：创建模拟推荐场景，估计推荐策略对点击率的因果效应- 可视化 DAG 并报告估计偏差来源 |
| **第 5 周**  | 高级因果方法          | - 工具变量（IV）、双重稳健估计（Doubly Robust）- 前/后验因果推断对比- Causal Embeddings 在推荐中的应用 | - 在模拟或公开数据集（如 Criteo）上用 IV 方法纠正用户反馈偏差，并对比纠正前后推荐效果- 将因果估计结果融入推荐得分，观察 Top-K 变化 |
| **第 6 周**  | 多臂赌博机（Bandits） | - Epsilon-Greedy、UCB、Thompson Sampling 原理- Contextual Bandit 框架- 在线 vs. 离线评估 | - 实现 ε-Greedy 与 UCB bandit，在模拟广告位推荐中比较累积奖励- 用离线日志数据做离线评估（IPS/CV） |
| **第 7 周**  | 强化学习基础          | - 马尔可夫决策过程（MDP）- Q-Learning、SARSA- Policy Gradient（REINFORCE） | - 在 OpenAI Gym（CartPole 或自定义新闻推荐环境）中分别实现 Q-Learning 与 REINFORCE，输出学习曲线- 用 PyTorch 搭建简易 DQN Agent |
| **第 8 周**  | 深度强化学习          | - DQN 进阶：目标网络、经验回放- DDPG / Actor-Critic 概念- Off-policy vs. On-policy | - 基于 Stable-Baselines3 改写一份 DQN 示例，并与自己手写版对比性能- 在新闻推荐环境中训练 DQN Agent，并分析策略变化 |
| **第 9 周**  | 因果视角下的强化学习  | - Causal RL：反事实估计、离线 RL（Off-policy Evaluation）- Counterfactual Policy Evaluation（CPE）指标- 在推荐中的应用场景 | - 在 RL for Recommendation 环境里加入因果评估指标（IPS、SNIPS、DR），对比不同策略的偏差- 撰写一段小报告，总结因果 RL 的价值与挑战 |
| **第 10 周** | 预训练大模型入门      | - Transformer 架构及自注意力- BERT / GPT 系列概览- Embedding 微调 vs. Prompt Learning | - 使用 Hugging Face Transformers 在小语料上微调 BERT，做内容补全或用户评论理解- 评估微调前后模型在下游任务（如用户偏好分类）上的效果 |
| **第 11 周** | RAG 与生成推荐系统    | - 检索增强生成（RAG）原理- 多模态 / 对话式推荐系统设计- 大模型作为用户画像与解释模块 | - 构建一个简单的 RAG Pipeline：文档检索 + GPT-style 生成，输出带理由的推荐列表- 集成到 Flask/FastAPI，提供 RESTful 接口 |
| **第 12 周** | 综合项目与部署优化    | - A/B 测试与在线评估设计- 模型监控与自动化流水线- 系统性能优化（批处理 vs. 实时） | - 打包一个小型推荐服务（含 API + 前端展示），内部集成因果评估与 RL 策略- 编写技术文档与 PPT，用于面试演示或团队分享 |